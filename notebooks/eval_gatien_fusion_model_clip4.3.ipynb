{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128#  8 for 13.8 gb usage, 6 for less than 12 gb usage\n",
    "NUM_CLASSES=27\n",
    "SEED=42\n",
    "\n",
    "CLIP_FEATURE_SIZE=768\n",
    "\n",
    "#testing\n",
    "# train_percentage=0.001\n",
    "# valid_percentage=0.001\n",
    "\n",
    "train_percentage=0.8\n",
    "valid_percentage=0.1\n",
    "#(test_percentage takes the rest)\n",
    "\n",
    "use_cropped=True\n",
    "\n",
    "google_colab=False\n",
    "force_cpu=False\n",
    "\n",
    "\n",
    "TEXT_EMBEDDING_DIM=300 #cannot be changed or retrain the text model\n",
    "VOCAB_SIZE=67465\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name=\"cropped_images\" if use_cropped else \"images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers pandas tqdm scikit-learn imageio matplotlib wget plotly dash unidecode tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel,CLIPFeatureExtractor\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "\n",
    "import imageio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from datetime import datetime\n",
    "import unidecode\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    # mount the drive where your dataset is availabledevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset\n",
    "\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "        with zipfile.ZipFile(filepath+images_name+'.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('datasets')\n",
    "\n",
    "    datasets_path=\"/content/datasets/\"\n",
    "    save_directory=\"/content/drive/MyDrive/Lessons/Models/multimodal_classification/\"\n",
    "\n",
    "else:\n",
    "    import wget\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "        output_directory=\"datasets\"\n",
    "        csv_zip = wget.download(\"https://nextcloud.its-tps.fr/s/BTpB4SC93NreZxg/download/csv_data.zip\",out=output_directory)\n",
    "        if use_cropped:\n",
    "            images_zip=wget.download(\"https://nextcloud.its-tps.fr/s/8dZMpfpDNnpaZ5P/download/cropped_images.zip\",out=output_directory)\n",
    "        else:\n",
    "            images_zip=wget.download(\"https://nextcloud.its-tps.fr/s/fgBxQczEAZ7ws8J/download/images.zip\",out=output_directory)\n",
    "        with zipfile.ZipFile(output_directory+'/csv_data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('datasets')\n",
    "        with zipfile.ZipFile(output_directory+'/'+images_name+'.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('datasets')\n",
    "    filepath=os.getcwd()+'/datasets/'\n",
    "    save_directory='../models/'\n",
    "    datasets_path=filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if force_cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv(filepath+'X_train.csv').fillna(\"\")\n",
    "y_train = pd.read_csv(filepath+'Y_train.csv').fillna(\"\")\n",
    "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
    "y_train=y_train.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "\n",
    "# Cleaning and Preprocessing Text\n",
    "CLEANR = re.compile('<.*?>') # delete html tag\n",
    "def clean_html(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "def clean_text(text):\n",
    "    text=clean_html(text)\n",
    "    # Remove special characters and numbers\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "    \n",
    "# Apply cleaning function to the 'designation' column\n",
    "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
    "X_train['description'] = X_train['description'].fillna('').apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['designation'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)# nearly 70 000 of vocab size, it seems too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df=pd.get_dummies(y_train, columns=['prdtypecode'])\n",
    "y_train_categorical = encoded_df.values.tolist()\n",
    "length=len(y_train_categorical)\n",
    "y_train_one_hot=pd.DataFrame(np.zeros((length,1),dtype=list),columns=[\"labels\"])\n",
    "for index,row in tqdm(enumerate(y_train_categorical)):\n",
    "    y_train_one_hot.loc[index, \"labels\"]=row\n",
    "train_df=pd.concat([X_train,y_train_one_hot],axis=1)\n",
    "y_train_out=np.argmax(y_train_categorical,1)\n",
    "y_train_out\n",
    "label_dict={}\n",
    "i=0\n",
    "while len(label_dict)<27:\n",
    "    key=y_train_out[i]\n",
    "    if key not in label_dict.keys():\n",
    "        value=y_train['prdtypecode'][i]\n",
    "        label_dict[key] = value\n",
    "    i+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataLoader(Dataset):\n",
    "    \"\"\"Title, Description and Image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        selected_df=self.df.iloc[idx]\n",
    "\n",
    "        image_name=\"image_\"+str(selected_df[\"imageid\"])+\"_product_\"+str(selected_df[\"productid\"])+\".jpg\"\n",
    "        filepath=os.path.join(self.image_dir,image_name)\n",
    "        image_arr = Image.open(filepath)\n",
    "        image_arr = image_arr.resize((224, 224))\n",
    "        image_arr=np.array(image_arr)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        designation_text=selected_df['designation']\n",
    "        description_text=selected_df['description']\n",
    "        if len(description_text)>10:\n",
    "            description_sequences = tokenizer.texts_to_sequences([description_text])\n",
    "            # Padding to max length of text\n",
    "            description = pad_sequences(description_sequences, maxlen=34)\n",
    "\n",
    "        else :description=np.zeros((1,34))\n",
    "        description=torch.from_numpy(description).to(device, dtype=int)\n",
    "\n",
    "        designation_sequences = tokenizer.texts_to_sequences([designation_text])\n",
    "        # Padding to max length of text\n",
    "    \n",
    "        designation =pad_sequences(designation_sequences, maxlen=34)\n",
    "        designation = torch.from_numpy(designation).to(device, dtype=int)\n",
    "        label=torch.tensor(selected_df['labels'], dtype=torch.float,device=device)\n",
    "      \n",
    "        return [designation,description,image_arr,label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cropped==True:\n",
    "    dataset=ImageTextDataLoader(train_df,datasets_path+\"/cropped_train\")\n",
    "else:\n",
    "    dataset=ImageTextDataLoader(train_df,datasets_path+\"/images/image_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(train_percentage * len(dataset))\n",
    "valid_size = int(valid_percentage * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size],generator=generator)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "dataloaders={\n",
    "    'train':train_dataloader,\n",
    "    'val':valid_dataloader,\n",
    "    'test':test_dataloader\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class Text_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(Text_model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(1, 512, (i, embedding_dim), padding=(0, 0))\n",
    "            for i in range(1,7)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * 6, CLIP_FEATURE_SIZE)\n",
    "        self.classif=nn.Linear(CLIP_FEATURE_SIZE,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv2d\n",
    "        conv_outputs = [nn.functional.relu(conv_block(x)).max(dim=3)[0].max(dim=2)[0] for conv_block in self.conv_blocks]# [0] to get only the values and not the indices ( in pos 1 )\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        # Dense Layer\n",
    "\n",
    "        # Flatten Layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dropout Layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = self.classif(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = Text_model(VOCAB_SIZE, TEXT_EMBEDDING_DIM, NUM_CLASSES)\n",
    "saved_state_dict = torch.load(\"/home/onyxia/work/multimodal_product_data_classification/models/Text_model_val_f1_0.747_epoch6.ckpt\")\n",
    "text_model.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.classif = nn.Sequential()#delete the classification head\n",
    "text_model.to(device)\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "Clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_features(images):\n",
    "    inputs = Clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    image_features = Clip_model.get_image_features(**inputs).to(device)\n",
    "    del inputs\n",
    "    return(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "   def __init__(self, input_dim, num_classes):\n",
    "       super(ClassificationHead, self).__init__()\n",
    "\n",
    "       self.head=nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(input_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,num_classes),\n",
    "       )\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.head(x)\n",
    "       return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ClassificationHead(CLIP_FEATURE_SIZE*2,NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/onyxia/work/multimodal_product_data_classification/models/CLIP_model_val_f1_0.747_epoch6.ckpt\"))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# All parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_f1_score(y_true, y_pred):\n",
    "  return f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(model,eval_dataloader):\n",
    "    preds_list=[]\n",
    "    model.eval()\n",
    "    # Iterate over data.\n",
    "    loop_on_eval_dataloader=tqdm(eval_dataloader,position=1,leave=False,ncols=800)\n",
    "    for designation,description,image_arr,labels in loop_on_eval_dataloader:\n",
    "        description_features=[]\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(description)):\n",
    "            #get the description that are not null\n",
    "                if torch.count_nonzero(description[i])>0:\n",
    "                    description_feature=text_model(description[i])\n",
    "                    description_features.append(description_feature.T)\n",
    "                else:\n",
    "                    description_features.append([])\n",
    "            images_features=get_images_features(image_arr).unsqueeze(2)\n",
    "            designation_features=text_model(designation.squeeze(1)).unsqueeze(2)\n",
    "\n",
    "            if len(description_features[0])>0:\n",
    "                designation_features[0]=(designation_features[0]+description_features[0])/2\n",
    "\n",
    "\n",
    "            #HERE implement fusion model of designation, description and image_arr\n",
    "            input_features=torch.cat((images_features[0] , designation_features[0])).unsqueeze(0)\n",
    "\n",
    "            for i in range(1,images_features.size(dim=0)):\n",
    "                # Perform the multiplication and append the result to the results array\n",
    "                if len(description_features[i])>0:\n",
    "\n",
    "                    designation_features[i]=(designation_features[i]+description_features[i])/2\n",
    "\n",
    "                input_feature=torch.cat((images_features[i] , designation_features[i]))\n",
    "                input_features = torch.cat((input_features, input_feature.unsqueeze(0)), dim=0)\n",
    "            input_features=input_features.squeeze(2)\n",
    "\n",
    "            preds = model(input_features)\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_eval = pd.read_csv(filepath+'X_test.csv').fillna(\"\")\n",
    "X_eval=X_eval.drop(columns=\"Unnamed: 0\")\n",
    "X_eval['designation'] = X_eval['designation'].fillna('').apply(clean_text)\n",
    "X_eval['description'] = X_eval['description'].fillna('').apply(clean_text)\n",
    "\n",
    "X_eval[\"labels\"]=np.zeros((len(X_eval[\"designation\"]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset=ImageTextDataLoader(X_eval,datasets_path+\"/images/image_test\")\n",
    "print(len(eval_dataset))\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
