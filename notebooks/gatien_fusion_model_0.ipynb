{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs/36LrUHfZ96kdtSvBpng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gatienc/multimodal_product_data_classification/blob/main/notebooks/gatien_fusion_model_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBXHMrlRNcJJ",
        "outputId": "c1e4c943-2184-4329-cb57-3b25fa0cec8f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Idea\n",
        "idea of this fusion model: our 2 model gives a vector of 27 element as output,\n",
        "we want to freeze both pretrained model, and just combine the output to classify better than on by one\n",
        "\n",
        "il faudrait qu'il y ai une couche qui fasse : $a*out_{image}+b*out_{text}$\n",
        "avec a et b des params entra√Ænables"
      ],
      "metadata": {
        "id": "FQcDRR-iSsMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "h5Yl2orsSbFN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=275#10.4/15GO\n",
        "num_classes=27\n",
        "\n",
        "train_percentage=0.80\n",
        "test_percentage=0.10\n",
        "valid_percentage=0.10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "Zqvc8uxrBNzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92AVOgi-PvYG",
        "outputId": "1fdfefd0-26c1-49c8-8a87-950557764d73"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm # for cool loading bar\n",
        "\n",
        "\n",
        "# import for NLP\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "OizAr-8tBUDx"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive where your dataset is availabledevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset\n",
        "\n",
        "if not os.path.exists('datasets'):\n",
        "  os.makedirs('datasets')\n",
        "  with zipfile.ZipFile(filepath+'images.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('datasets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81QaCSR6Baiv",
        "outputId": "9a2e953c-a192-4777-aa80-28fdfb74e206"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "VuML9NpyBXKS"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "U5JZcRydJSq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv(filepath+'X_train.csv')\n",
        "y_train = pd.read_csv(filepath+'Y_train.csv')\n",
        "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
        "y_train=y_train.drop(columns=\"Unnamed: 0\")\n"
      ],
      "metadata": {
        "id": "XLP6T2xjDCDw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning and Preprocessing Text\n",
        "\n",
        "\n",
        "CLEANR = re.compile('<.*?>') # delete html tag\n",
        "\n",
        "def clean_html(raw_html):\n",
        "  cleantext = re.sub(CLEANR, '', raw_html)\n",
        "  return cleantext\n",
        "def clean_special(text):\n",
        "    text = re.sub(r'[^a-z% ]', '', text)\n",
        "    return(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    text=re.sub('\\d', '%', text)#replace all digits by %\n",
        "    text = unidecode(text)\n",
        "    text=clean_html(text)\n",
        "    text=clean_special(text)\n",
        "    #clean stopwords\n",
        "    text_list=text.split()\n",
        "    return([word for word in text_list if word not in final_stopwords_list])\n",
        "\n",
        "\n",
        "\n",
        "final_stopwords_list = stopwords.words('english') + stopwords.words('french')+stopwords.words('german')# Fr,En,De Stopwords\n",
        "\n",
        "\n",
        "# Apply cleaning function to the 'designation' column\n",
        "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
        "X_train['description'] = X_train['description'].fillna('').apply(clean_text)\n"
      ],
      "metadata": {
        "id": "JzDGmO-0C-lt"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 150)\n",
        "X_train.designation.head(50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrogdtIPJ2B7",
        "outputId": "a9735a8f-0122-42ae-902d-2d493926d7ca"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                 [olivia, personalisiertes, notizbuch, %%%, seiten, punktraster, ca, din, a%, rosendesign]\n",
              "1     [journal, arts, ndeg, %%%, %%%%%%%%, lart, marche, salon, dart, asiatique, paris, jacques, barrere, francois, perrier, reforme, ventes, encheres, ...\n",
              "2                                                                    [grand, stylet, ergonomique, bleu, gamepad, nintendo, wii, u, speedlink, pilot, style]\n",
              "3                                                                                           [peluche, donald, europe, disneyland, %%%%, marionnette, doigt]\n",
              "4                                                                                                                                          [guerre, tuques]\n",
              "5                                                                                 [afrique, contemporaine, ndeg, %%%, hiver, %%%%, dossier, japon, afrique]\n",
              "6                                                                                                                    [christof, e, bildungsprozessen, spur]\n",
              "7                                                         [conquerant, sept, cahier, couverture, polypro, %%%, x, %%%, mm, %%, pages, %%g, seyes, incolore]\n",
              "8                                                                                                                 [puzzle, scoobydoo, poster, %x%%, pieces]\n",
              "9                                                                   [tente, pliante, v%s%pro, pvc, blanc, %, x, %m%%, longueur, %m%%, largeur, %, blanc, h]\n",
              "10                                                                                                               [eames, inspired, sxw, chair, pink, black]\n",
              "11                                                                                      [fauteuil, chesterfield, brenton, %%%%, cuir, buffle, vert, empire]\n",
              "12                                                                     [peaceable, kingdom, wheres, bear, hide, find, stacking, block, game, %, year, olds]\n",
              "13                                                                                                                   [paire, voilages, imprimes, fantaisie]\n",
              "14    [matelas, memoire, forme, %%%x%%%, x, %%, cm, tres, ferme, dehoussable, housse, lavable, %, zones, confort, noyau, poli, lattex, hr, derniere, gen...\n",
              "15                                                                                                   [zenith, pince, agrafeuse, %%%, ndeg%%, coloris, noir]\n",
              "16                                                                                                 [walter, scott, oeuvres, completes, tomes, %%%%, %%, %%]\n",
              "17                                                                                                               [mod, podge, dishwasher, safe, gloss, %oz]\n",
              "18                                                                                  [power, rangers, rouge, force, mystic, figurine, transformable, %%, cm]\n",
              "19    [monde, illustre, ndeg, %%%%, %%%%%%%%, lemprunt, victoire, angleterre, meeting, guildhall, lloyd, george, bonar, law, mac, kenna, conference, all...\n",
              "20                                                                                                            [kit, desinfection, piscines, enfants, %%%%%]\n",
              "21                                                                                                                         [glitter, beach, barbie, barbie]\n",
              "22                                                                                                [seigneur, anneaux, figurine, plomb, peindre, socle, sam]\n",
              "23                                                                                                                            [vehicule, star, wars, aast%]\n",
              "24                                 [mini, wifi, %%%p, camera, drone, rc, quadcopter, %%, ghz, %ch, %axis, gyro, %d, ufo, fpv, rc, %%%%, cocoworldgenerique]\n",
              "25                                                                                                                       [dsi, chargeur, sacoche, %%, jeux]\n",
              "26                                        [modele, voiture, %pcs, alliage, metallique, %%in, jante, roue, %%%, traxxas, trx%, hsp, scx%%, d%%, rc, voiture]\n",
              "27                                                              [fabercastell, lot, %, crayons, couleur, polychromos, mine, %%mm, l%%huile, vert, emeraude]\n",
              "28                                                            [univers, ndeg, %%%, %%%%%%%%, france, paris, %%, septembre, %%%%, lac, protestation, joseph]\n",
              "29                                                                                        [dragon, ball, super, bt%%%%, c%%, premisses, peur, peu, commune]\n",
              "30      [lampe, lecture, rechargeable, led, lampe, beaute, lampe, protection, yeux, lampe, froide, col, cygne, lampe, moderne, lampe, bureau, rechargeable]\n",
              "31                                                                                                                                         [xmen, %, movie]\n",
              "32                                                                                              [%, cagettes, rangement, happy, life, %%, x, %%, cm, beige]\n",
              "33                  [lindner, %%%%%%%%ce, coin, case, nera, xl, %, trays, black, coin, inserts, %%, rectangular, compartments, coinscoin, capsules, %%, mm]\n",
              "34                                                                                                                                            [lord, rings]\n",
              "35                                                         [decoration, noel, christmas, snowman, kitchen, table, chair, covers, holiday, home, decoration]\n",
              "36                                                                [%%, spots, encastrable, orientable, blanc, gu%%, led, %w, eqv, %%w, blanc, froid, %%%%k]\n",
              "37                                              [nouveau, %%h%%, echelle, alloy, mini, pull, back, voiturette, w, clubs, diecast, jouets, modele, vehicule]\n",
              "38                                                                              [%pcs, decor, coussin, independance, style, jeter, covers, taie, doreiller]\n",
              "39                                                                                                                [bouee, gonflable, river, tube, oogarden]\n",
              "40           [mini, console, jeux, double, joueurs, %, bits, console, jeux, video, %%%, enfance, jeux, classiques, av, family, tv, controleur, jeux, retro]\n",
              "41                                                                                                                        [presentoir, biscuits, chocolats]\n",
              "42                                                                                          [pilot, mr, stylo, plume, moyenne, design, crocodile, noirbleu]\n",
              "43                                                       [non, toxique, effacer, slime, coul, beau, melange, nuage, slime, relief, enfants, jouets, stress]\n",
              "44                                                                       [diagrammes, ndeg, %%, %%%%%%%%, siecle, petrole, guillerme, claudehenri, rocquet]\n",
              "45                                                                           [rideau, oeillets, plastique, polyester, uni, essentiel, lin, %%%, x, %%%, cm]\n",
              "46                                                                                                     [vassiviere, enlimousin, jardin, art, lieu, memoire]\n",
              "47                                                                                                        [shin, masoukishin, panzer, warfare, import, jap]\n",
              "48                                                                                                               [dragon, ball, z, serie, %, %%, francaise]\n",
              "49                                                [%%%pcs, premium, batons, bracelets, neon, light, glowing, party, favors, rallye, raves, %%%%, cocoworld]\n",
              "Name: designation, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train['designation'])\n",
        "sequences = tokenizer.texts_to_sequences(X_train['designation'])\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n",
        "\n",
        "# Assuming the number of unique words in the tokenizer plus 1 is vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)# nearly 70 000 of vocab size, it seems too much\n",
        "\n",
        "# Split data into training and validation set (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, y_train, test_size=0.2,shuffle=False)\n",
        "y_train=y_train[\"prdtypecode\"].tolist()\n",
        "y_val=y_val[\"prdtypecode\"].tolist()\n",
        "\n",
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytY0WIWtJ1VH",
        "outputId": "3e5cde68-06bb-437c-9d90-e3293619940f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data loading"
      ],
      "metadata": {
        "id": "9yrOg2yB3UUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTextDataLoader():\n",
        "    \"\"\"Title, Description and Image dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            image_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.image_dir,self.landmarks_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks], dtype=float).reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return [self.df,sample]\n",
        "\n",
        "\n",
        "    def image_search(self,id):\n",
        "        \"\"\"\n",
        "        Return the path of the image from the productID\n",
        "\n",
        "        still not optimized o(n**2) (did'nt found better way)\n",
        "        \"\"\"\n",
        "\n",
        "        def wich_patterns(patterns_list,string):\n",
        "            \"\"\"\n",
        "            returns the the pattern (from patterns) that match string if exists\n",
        "            \"\"\"\n",
        "            for index,pattern in enumerate(patterns_list):\n",
        "                if pattern in string:\n",
        "                    return([True, index])\n",
        "            else: return([False,\"\"])\n",
        "\n",
        "        if type(id)==int:\n",
        "            id=[id]\n",
        "\n",
        "        directory=self.image_dir+\"/images\"\n",
        "        #get the list of string patterns to look for in the file of the folder\n",
        "        patterns=np.ones(range(len(id)))\n",
        "        for i in range(len(id)):\n",
        "            patterns.append(\"_\"+str(id)+\".\")\n",
        "\n",
        "        arr_str = [''] * len(id)\n",
        "\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(file_path)\n",
        "                with open(file_path, 'r') as f:\n",
        "                    in_pattern,pattern_index= wich_patterns(patterns,os.path.abspath(file_path))\n",
        "                    if in_pattern:\n",
        "                        arr_str[pattern_index]=os.path.abspath(file_path)\n",
        "        if \"\" not in arr_str:\n",
        "            return(arr_str)\n",
        "        else :\n",
        "            raise ValueError(\"the array string is not fully filled, maybe an image description is lacking in the file, or problem in the dataloader\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2Aq5ejkMivSe"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize(256),\n",
        "            torchvision.transforms.CenterCrop(224),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "       ])"
      ],
      "metadata": {
        "id": "uAnimxd2ILAg"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=ImageTextDataLoader(\"/content/drive/MyDrive/datasets/multimodal_product_classification/X_train.csv\",\"/content/drive/MyDrive/datasets/images/image_train\",transform)"
      ],
      "metadata": {
        "id": "KRy2RCUhHwaw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(train_percentage * len(dataset))\n",
        "valid_size = int(test_percentage * len(dataset))\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "dataloaders={\n",
        "    'train':train_dataloader,\n",
        "    'val':valid_dataloader,\n",
        "    'test':test_dataloader\n",
        "    }\n",
        "\n",
        "model = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Replace the last fully-connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, num_classes) # num_classes is the number of classes in your dataset\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Lessons/Models/multimodal_classification/Resnet16-best10.ckpt\",map_location=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0eZPtjTBdoy",
        "outputId": "e9c0687a-c234-4cd9-85cf-66c0ebf7974b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 79.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "e252R4xYEoeu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a178cc10-fda1-493b-fbd1-b09c36b02291"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ImageTextDataLoader' object has no attribute 'landmarks_frame'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-83d946607118>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-5404e5fd6af9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmarks_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmarks_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageTextDataLoader' object has no attribute 'landmarks_frame'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_batch"
      ],
      "metadata": {
        "id": "TmdoMWBSE73B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}