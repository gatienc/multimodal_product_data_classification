{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "CLIP_FEATURE_SIZE=768\n"
      ],
      "metadata": {
        "id": "hUwysigOttvt"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXpZjiYTSoiE",
        "outputId": "cd530685-842a-4c38-b828-7bc26e8e0b6c"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "3K54S3DVU0Qw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# import for NLP\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "aKLO2oDtU0Qy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLOe_dhqU0Qz",
        "outputId": "3146af80-2e31-416f-f98d-d73b94ec55fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount the drive where your dataset is available\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "Vt4WTd7YU0Q0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv(filepath+'X_train.csv')\n",
        "y_train = pd.read_csv(filepath+'Y_train.csv')\n",
        "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
        "y_train=y_train.drop(columns=\"Unnamed: 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "chxen8jQU0Q1"
      },
      "outputs": [],
      "source": [
        "# Cleaning and Preprocessing Text\n",
        "def clean_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syt9HpO2U0Q2",
        "outputId": "7b3d2c3f-08f1-49eb-c7b7-a34cdc2cdb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69189\n"
          ]
        }
      ],
      "source": [
        "# Apply cleaning function to the 'designation' column\n",
        "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train['designation'])\n",
        "sequences = tokenizer.texts_to_sequences(X_train['designation'])\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n",
        "\n",
        "# Assuming the number of unique words in the tokenizer plus 1 is vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)# nearly 70 000 of vocab size, it seems too much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "0vSBCqyaU0Q3"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation set (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, y_train, test_size=0.2,shuffle=False)\n",
        "y_train=y_train[\"prdtypecode\"].tolist()\n",
        "y_val=y_val[\"prdtypecode\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)\n"
      ],
      "metadata": {
        "id": "Tt6NKIafH2Zw"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.inputs = X\n",
        "        self.labels = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.inputs[idx]).to(device)\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.float).to(device)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "dGdDkF1McYMq"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "batch_size=128"
      ],
      "metadata": {
        "id": "Bv7-J5jNM4ko"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=TextDataset(X_train,y_train_categorical)\n",
        "train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset=TextDataset(X_val,y_val_categorical)\n",
        "val_loader=DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Ne8BU9YReV1V"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPs8yDGU0Q3"
      },
      "source": [
        "# Model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "9ZhLwkJBU0Q5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the model\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Conv2d(1, 512, (i, embedding_dim), padding=(0, 0))\n",
        "            for i in range(1,7)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512 * 6, CLIP_FEATURE_SIZE)\n",
        "        self.classif=nn.Linear(CLIP_FEATURE_SIZE,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv2d\n",
        "        conv_outputs = [nn.functional.relu(conv_block(x)).max(dim=3)[0].max(dim=2)[0] for conv_block in self.conv_blocks]# [0] to get only the values and not the indices ( in pos 1 )\n",
        "        x = torch.cat(conv_outputs, dim=1)\n",
        "        # Dense Layer\n",
        "\n",
        "        # Flatten Layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Dropout Layer\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.classif(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U8waN8YU0Q7",
        "outputId": "be63357d-7d84-4d23-9aa5-82ce1ddd8fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_classifier(\n",
            "  (embedding): Embedding(69189, 300)\n",
            "  (conv_blocks): ModuleList(\n",
            "    (0): Conv2d(1, 512, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 512, kernel_size=(2, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 512, kernel_size=(3, 300), stride=(1, 1))\n",
            "    (3): Conv2d(1, 512, kernel_size=(4, 300), stride=(1, 1))\n",
            "    (4): Conv2d(1, 512, kernel_size=(5, 300), stride=(1, 1))\n",
            "    (5): Conv2d(1, 512, kernel_size=(6, 300), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (classif): Linear(in_features=768, out_features=27, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "num_classes = 27\n",
        "model = CNN_classifier(vocab_size, embedding_dim, num_classes)\n",
        "\n",
        "# Convert the model to CUDA if available\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# Print the model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "1ohTkfsnU0Q9",
        "outputId": "14dcdf25-e530-44c9-810a-120d309c9312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-155-374e589337f1>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-155-374e589337f1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def train_model(model,train_loader,val_loader,num_epochs=10)  # Train the model\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,val_loader,num_epochs=10):  # Train the model\n",
        "  val_f1=0\n",
        "  max_val_f1=0\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      model.train()\n",
        "      for inputs, labels in tqdm(train_loader,desc=f\"Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {val_f1:.4f}\"):\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          outputs=outputs.squeeze(0)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          val_preds = []\n",
        "          val_labels = []\n",
        "          for val_inputs, val_labels_batch in val_loader:\n",
        "              val_outputs = model(val_inputs)\n",
        "              val_preds.append(val_outputs.cpu())\n",
        "              val_labels.append(val_labels_batch.cpu())\n",
        "\n",
        "      val_preds = torch.cat(val_preds, dim=0)\n",
        "      val_labels = torch.cat(val_labels, dim=0)\n",
        "\n",
        "    #   print(f'{val_labels=}')\n",
        "    #   print(f'{torch.argmax(val_preds, dim=1)=}')\n",
        "\n",
        "      val_f1 = f1_score(torch.argmax(val_labels,dim=1), torch.argmax(val_preds, dim=1), average='macro')\n",
        "      if val_f1>max_val_f1:\n",
        "        max_val_f1=val_f1\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/Lessons/Models/multimodal_classification/' + 'Text_model_val_f1_{:.3f}_epoch{}.ckpt'.format(val_f1,epoch))\n",
        "        best_model=model.copy()\n",
        "  return(best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=train_model(model,train_loader,val_loader,num_epochs=10)"
      ],
      "metadata": {
        "id": "8u5be1oU7yaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part"
      ],
      "metadata": {
        "id": "CNAorMrKdzAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(filepath+'X_test.csv')\n",
        "X_test=X_test.drop(columns=\"Unnamed: 0\")\n",
        "X_test['designation'] = X_test['designation'].fillna('').apply(clean_text)\n",
        "sequences = tokenizer.texts_to_sequences(X_test['designation'])\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n"
      ],
      "metadata": {
        "id": "coEIf5uneNo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "cncW6xlweu2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_classifier(vocab_size, embedding_dim, num_classes)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Lessons/Models/multimodal_classification/Text_model_val_f1_0.734_epoch4.ckpt'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "eM-WMkjfgKxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "  for input in tqdm(data):\n",
        "      input=torch.from_numpy(input)\n",
        "      input=input.to(device)\n",
        "      input=input.unsqueeze(0)\n",
        "      val_outputs = model(input)[0]\n",
        "      val_outputs=val_outputs.detach().cpu().numpy()\n",
        "      val_preds.append(val_outputs)"
      ],
      "metadata": {
        "id": "pqWFN9t5dxjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds"
      ],
      "metadata": {
        "id": "fFBACRoW5LMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds=np.argmax(val_preds,axis=1)"
      ],
      "metadata": {
        "id": "LsEZpp8_hZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds=label_encoder.inverse_transform(val_preds)"
      ],
      "metadata": {
        "id": "GEWqgCPO6muD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds=pd.DataFrame(val_preds)\n",
        "df_preds=df_preds.set_index(df_preds.index+84916)"
      ],
      "metadata": {
        "id": "PixKkCRskS70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds"
      ],
      "metadata": {
        "id": "F1dGk5Tb7Ah7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds.to_csv(\"eval_text_designation.csv\")"
      ],
      "metadata": {
        "id": "Skhnz__f4xde"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}