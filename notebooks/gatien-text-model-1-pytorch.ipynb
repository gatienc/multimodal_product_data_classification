{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import for NLP\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount the drive where your dataset is available\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv(filepath+'X_train.csv')\n",
    "y_train = pd.read_csv(filepath+'Y_train.csv')\n",
    "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
    "y_train=y_train.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and Preprocessing Text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function to the 'designation' column\n",
    "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['designation'])\n",
    "sequences = tokenizer.texts_to_sequences(X_train['designation'])\n",
    "\n",
    "# Padding to max length of text\n",
    "data = pad_sequences(sequences, maxlen=34)\n",
    "\n",
    "# Assuming the number of unique words in the tokenizer plus 1 is vocab_size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation set (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(1, 512, (i, embedding_dim), padding=(i-1, 0))\n",
    "            for i in range(1, 7)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv2d\n",
    "        conv_outputs = [nn.functional.relu(conv_block(x)).max(dim=3)[0] for conv_block in self.conv_blocks]\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model\n",
    "embedding_dim = 300\n",
    "num_classes = 27\n",
    "model = CNN_classifier(vocab_size, embedding_dim, num_classes)\n",
    "\n",
    "# Convert the model to CUDA if available\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train_encoded)\n",
    "y_val_categorical = to_categorical(y_val_encoded)\n",
    "\n",
    "# Adjust the final layer of the model to have as many units as there are unique classes\n",
    "num_classes = y_train_categorical.shape[1]\n",
    "model.layers[-1].units = num_classes\n",
    "\n",
    "# Compile the model again\n",
    "f1_score = metrics.F1Score(average='macro')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[f1_score])\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_categorical, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val_categorical))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        for val_inputs, val_labels_batch in val_loader:\n",
    "            val_inputs, val_labels_batch = val_inputs.to(device), val_labels_batch.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_preds.append(val_outputs.cpu())\n",
    "            val_labels.append(val_labels_batch.cpu())\n",
    "\n",
    "    val_preds = torch.cat(val_preds, dim=0)\n",
    "    val_labels = torch.cat(val_labels, dim=0)\n",
    "    \n",
    "    val_f1 = f1_metric(val_preds, val_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {val_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
