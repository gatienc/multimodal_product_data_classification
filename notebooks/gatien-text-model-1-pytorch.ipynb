{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "id": "3K54S3DVU0Qw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# import for NLP\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "aKLO2oDtU0Qy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLOe_dhqU0Qz",
        "outputId": "b0205c9f-b75c-4da5-a25b-619b9510a07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount the drive where your dataset is available\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "Vt4WTd7YU0Q0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv(filepath+'X_train.csv')\n",
        "y_train = pd.read_csv(filepath+'Y_train.csv')\n",
        "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
        "y_train=y_train.drop(columns=\"Unnamed: 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "chxen8jQU0Q1"
      },
      "outputs": [],
      "source": [
        "# Cleaning and Preprocessing Text\n",
        "def clean_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syt9HpO2U0Q2",
        "outputId": "25eb658c-e0fb-45dd-cc74-81e64c8b63e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69189\n"
          ]
        }
      ],
      "source": [
        "# Apply cleaning function to the 'designation' column\n",
        "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train['designation'])\n",
        "sequences = tokenizer.texts_to_sequences(X_train['designation'])\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n",
        "\n",
        "# Assuming the number of unique words in the tokenizer plus 1 is vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)# nearly 70 000 of vocab size, it seems too much\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "0vSBCqyaU0Q3"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation set (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, y_train, test_size=0.2,shuffle=False)\n",
        "y_train=y_train[\"prdtypecode\"].tolist()\n",
        "y_val=y_val[\"prdtypecode\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.inputs = X\n",
        "        self.labels = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.inputs[idx])\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "dGdDkF1McYMq"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=TextDataset(X_train,y_train)"
      ],
      "metadata": {
        "id": "Ne8BU9YReV1V"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader=TextDataset(X_val,y_val)"
      ],
      "metadata": {
        "id": "VH4fPv5PfefJ"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPs8yDGU0Q3"
      },
      "source": [
        "# Model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "9ZhLwkJBU0Q5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the model\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Conv2d(1, 512, (i, embedding_dim), padding=(0, 0))\n",
        "            for i in range(1,7)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512 * 6, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv2d\n",
        "        conv_outputs = [nn.functional.relu(conv_block(x)).max(dim=3)[0].max(dim=2)[0] for conv_block in self.conv_blocks]# [0] to get only the values and not the indices ( in pos 1 )\n",
        "        #print(f'{conv_outputs[0]=}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        x = torch.cat(conv_outputs, dim=1)\n",
        "\n",
        "\n",
        "        # Dense Layer\n",
        "        # Flatten Layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Dropout Layer\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U8waN8YU0Q7",
        "outputId": "b1b20d72-431c-47f2-8978-a69ef23ee641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_classifier(\n",
            "  (embedding): Embedding(69189, 300)\n",
            "  (conv_blocks): ModuleList(\n",
            "    (0): Conv2d(1, 512, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 512, kernel_size=(2, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 512, kernel_size=(3, 300), stride=(1, 1))\n",
            "    (3): Conv2d(1, 512, kernel_size=(4, 300), stride=(1, 1))\n",
            "    (4): Conv2d(1, 512, kernel_size=(5, 300), stride=(1, 1))\n",
            "    (5): Conv2d(1, 512, kernel_size=(6, 300), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=3072, out_features=27, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the model\n",
        "embedding_dim = 300\n",
        "num_classes = 27\n",
        "model = CNN_classifier(vocab_size, embedding_dim, num_classes)\n",
        "\n",
        "# Convert the model to CUDA if available\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "from sklearn.metrics import f1_score\n",
        "f1_metric = lambda preds, labels: f1_score(labels, torch.argmax(preds, dim=1), average='macro')\n",
        "\n",
        "\n",
        "# Print the model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "zQYw7V7UU0Q8"
      },
      "outputs": [],
      "source": [
        "#batch_size=32\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "PhImQtGZU0Q9"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_vec=torch.zeros(1,34,dtype=int).to(device)\n",
        "print(dummy_vec.shape)\n",
        "model(dummy_vec)"
      ],
      "metadata": {
        "id": "s7HznRkWqGTK",
        "outputId": "f1acc30f-545a-43de-a167-8657e619859d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 34])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.8928e-02,  2.4975e-01, -2.9169e-01,  7.1216e-03,  6.0109e-01,\n",
              "         -7.2056e-02,  3.0336e-01,  1.7174e-01, -2.6807e-01,  5.4169e-01,\n",
              "         -2.9937e-01,  2.1666e-01, -3.0565e-01,  6.1745e-03, -1.3642e-02,\n",
              "         -4.3603e-01, -2.8363e-01, -1.6562e-01,  8.1895e-02, -3.4851e-01,\n",
              "         -3.5221e-04,  1.3763e-01,  5.9315e-02, -1.4104e-01, -7.5252e-02,\n",
              "          4.8146e-01, -3.9103e-02]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "1ohTkfsnU0Q9",
        "outputId": "76ce0d4a-4bcb-4d85-d3b4-287a65a04373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape=torch.Size([1, 34])\n",
            "outputs.shape=torch.Size([27])\n",
            "test\n",
            "inputs.shape=torch.Size([1, 34])\n",
            "outputs.shape=torch.Size([27])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-372-120ea6390ce3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{outputs.shape=}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 2280 is out of bounds."
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 1  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        inputs=inputs.unsqueeze(0)\n",
        "        print(f'{inputs.shape=}')\n",
        "        outputs = model(inputs)\n",
        "        outputs=outputs.squeeze(0)\n",
        "\n",
        "        print(f'{outputs.shape=}')\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(\"test\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        for val_inputs, val_labels_batch in val_loader:\n",
        "            val_inputs, val_labels_batch = val_inputs.to(device), val_labels_batch.to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_preds.append(val_outputs.cpu())\n",
        "            val_labels.append(val_labels_batch.cpu())\n",
        "\n",
        "    val_preds = torch.cat(val_preds, dim=0)\n",
        "    val_labels = torch.cat(val_labels, dim=0)\n",
        "\n",
        "    val_f1 = f1_metric(val_preds, val_labels)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {val_f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}