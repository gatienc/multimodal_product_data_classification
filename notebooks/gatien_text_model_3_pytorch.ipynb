{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gatienc/multimodal_product_data_classification/blob/main/gatien_text_model_3_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K06hlE-s8dZw"
      },
      "source": [
        "*diff* : trying training on description too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hUwysigOttvt"
      },
      "outputs": [],
      "source": [
        "CLIP_FEATURE_SIZE=768\n",
        "google_colab=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/mamba/lib/python3.10/site-packages (4.36.2)\n",
            "Requirement already satisfied: pandas in /opt/mamba/lib/python3.10/site-packages (2.2.0)\n",
            "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (4.66.1)\n",
            "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: imageio in /opt/mamba/lib/python3.10/site-packages (2.33.1)\n",
            "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: wget in /opt/mamba/lib/python3.10/site-packages (3.2)\n",
            "Requirement already satisfied: plotly in /opt/mamba/lib/python3.10/site-packages (5.18.0)\n",
            "Requirement already satisfied: dash in /opt/mamba/lib/python3.10/site-packages (2.14.2)\n",
            "Requirement already satisfied: unidecode in /opt/mamba/lib/python3.10/site-packages (1.3.8)\n",
            "Requirement already satisfied: tensorflow in /opt/mamba/lib/python3.10/site-packages (2.15.0.post1)\n",
            "Requirement already satisfied: filelock in /opt/mamba/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/mamba/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/mamba/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/mamba/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /opt/mamba/lib/python3.10/site-packages (from imageio) (10.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/mamba/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /opt/mamba/lib/python3.10/site-packages (from dash) (3.0.1)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /opt/mamba/lib/python3.10/site-packages (from dash) (3.0.1)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /opt/mamba/lib/python3.10/site-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /opt/mamba/lib/python3.10/site-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /opt/mamba/lib/python3.10/site-packages (from dash) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/mamba/lib/python3.10/site-packages (from dash) (4.9.0)\n",
            "Requirement already satisfied: retrying in /opt/mamba/lib/python3.10/site-packages (from dash) (1.3.4)\n",
            "Requirement already satisfied: ansi2html in /opt/mamba/lib/python3.10/site-packages (from dash) (1.9.1)\n",
            "Requirement already satisfied: nest-asyncio in /opt/mamba/lib/python3.10/site-packages (from dash) (1.5.8)\n",
            "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from dash) (68.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /opt/mamba/lib/python3.10/site-packages (from dash) (7.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/mamba/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/mamba/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /opt/mamba/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/mamba/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash) (2.1.2)\n",
            "Requirement already satisfied: click>=8.1.3 in /opt/mamba/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /opt/mamba/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash) (1.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/mamba/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/mamba/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/mamba/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/mamba/lib/python3.10/site-packages (from Werkzeug<3.1->dash) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/mamba/lib/python3.10/site-packages (from importlib-metadata->dash) (3.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/mamba/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/mamba/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/mamba/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/mamba/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/mamba/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers pandas tqdm scikit-learn imageio matplotlib wget plotly dash unidecode tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3K54S3DVU0Qw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# import for NLP\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import zipfile\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import unidecode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aKLO2oDtU0Qy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "if google_colab:\n",
        "# mount the drive where your dataset is availabledevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  filepath='/content/drive/MyDrive/datasets/multimodal_product_classification/' # add your own path. Where to save the dataset\n",
        "\n",
        "  if not os.path.exists('datasets'):\n",
        "    os.makedirs('datasets')\n",
        "    with zipfile.ZipFile(filepath+images_name+'.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('datasets')\n",
        "\n",
        "  datasets_path=\"/content/datasets/\"\n",
        "  save_directory=\"/content/drive/MyDrive/Lessons/Models/multimodal_classification/\"\n",
        "else:\n",
        "    import wget\n",
        "    if not os.path.exists('datasets'):\n",
        "        os.makedirs('datasets')\n",
        "        output_directory=\"datasets\"\n",
        "        csv_zip = wget.download(\"https://nextcloud.its-tps.fr/s/BTpB4SC93NreZxg/download/csv_data.zip\",out=output_directory)\n",
        "        with zipfile.ZipFile(output_directory+'/csv_data.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('datasets')\n",
        "    filepath=os.getcwd()+'/datasets/'\n",
        "    save_directory='../models/'\n",
        "    datasets_path=filepath\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Vt4WTd7YU0Q0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv(filepath+'X_train.csv')\n",
        "y_train = pd.read_csv(filepath+'Y_train.csv')\n",
        "X_train=X_train.drop(columns=\"Unnamed: 0\")\n",
        "y_train=y_train.drop(columns=\"Unnamed: 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "chxen8jQU0Q1"
      },
      "outputs": [],
      "source": [
        "# Cleaning and Preprocessing Text\n",
        "CLEANR = re.compile('<.*?>') # delete html tag\n",
        "\n",
        "def clean_html(raw_html):\n",
        "  cleantext = re.sub(CLEANR, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "def clean_text(text):\n",
        "    text=clean_html(text)\n",
        "    # Remove special characters and numbers\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = re.sub(r'[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "syt9HpO2U0Q2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67465\n"
          ]
        }
      ],
      "source": [
        "# Apply cleaning function to the 'designation' column\n",
        "X_train['designation'] = X_train['designation'].fillna('').apply(clean_text)\n",
        "X_train['description'] = X_train['description'].fillna('').apply(clean_text)\n",
        "description_list=[]\n",
        "description_index=[]\n",
        "for index,element in enumerate(X_train['description']):\n",
        "    if len(element)>10:\n",
        "        # print(\"element\",element)\n",
        "        description_list.append(element)\n",
        "        description_index.append(index)\n",
        "y_train_description=[]\n",
        "for index in description_index:\n",
        "    y_train_description.append(y_train[\"prdtypecode\"][int(index)])\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train['designation'])\n",
        "sequences = tokenizer.texts_to_sequences(pd.concat([X_train['designation'], pd.Series(description_list)], axis=0))\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n",
        "\n",
        "# Assuming the number of unique words in the tokenizer plus 1 is vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)# nearly 70 000 of vocab size, it seems too much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0vSBCqyaU0Q3"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation set (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(data,pd.concat([y_train, pd.DataFrame(y_train_description,columns=['prdtypecode'])], axis=0) , test_size=0.2,shuffle=True)\n",
        "y_train=y_train[\"prdtypecode\"].tolist()\n",
        "y_val=y_val[\"prdtypecode\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Tt6NKIafH2Zw"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dGdDkF1McYMq"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.inputs = X\n",
        "        self.labels = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.inputs[idx]).to(device)\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.float).to(device)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bv7-J5jNM4ko"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 300\n",
        "batch_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ne8BU9YReV1V"
      },
      "outputs": [],
      "source": [
        "train_dataset=TextDataset(X_train,y_train_categorical)\n",
        "train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset=TextDataset(X_val,y_val_categorical)\n",
        "val_loader=DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPs8yDGU0Q3"
      },
      "source": [
        "# Model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9ZhLwkJBU0Q5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the model\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Conv2d(1, 512, (i, embedding_dim), padding=(0, 0))\n",
        "            for i in range(1,7)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512 * 6, CLIP_FEATURE_SIZE)\n",
        "        self.classif=nn.Linear(CLIP_FEATURE_SIZE,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv2d\n",
        "        conv_outputs = [nn.functional.relu(conv_block(x)).max(dim=3)[0].max(dim=2)[0] for conv_block in self.conv_blocks]# [0] to get only the values and not the indices ( in pos 1 )\n",
        "        x = torch.cat(conv_outputs, dim=1)\n",
        "        # Dense Layer\n",
        "\n",
        "        # Flatten Layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Dropout Layer\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.classif(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1U8waN8YU0Q7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN_classifier(\n",
            "  (embedding): Embedding(67465, 300)\n",
            "  (conv_blocks): ModuleList(\n",
            "    (0): Conv2d(1, 512, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 512, kernel_size=(2, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 512, kernel_size=(3, 300), stride=(1, 1))\n",
            "    (3): Conv2d(1, 512, kernel_size=(4, 300), stride=(1, 1))\n",
            "    (4): Conv2d(1, 512, kernel_size=(5, 300), stride=(1, 1))\n",
            "    (5): Conv2d(1, 512, kernel_size=(6, 300), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (classif): Linear(in_features=768, out_features=27, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "num_classes = 27\n",
        "model = CNN_classifier(vocab_size, embedding_dim, num_classes)\n",
        "\n",
        "# Convert the model to CUDA if available\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# Print the model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1ohTkfsnU0Q9"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_loader,val_loader,num_epochs=10):  # Train the model\n",
        "  val_f1=0\n",
        "  max_val_f1=0\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      model.train()\n",
        "      for inputs, labels in tqdm(train_loader,desc=f\"Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {val_f1:.4f}\"):\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          outputs=outputs.squeeze(0)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          val_preds = []\n",
        "          val_labels = []\n",
        "          for val_inputs, val_labels_batch in val_loader:\n",
        "              val_outputs = model(val_inputs)\n",
        "              val_preds.append(val_outputs.cpu())\n",
        "              val_labels.append(val_labels_batch.cpu())\n",
        "\n",
        "      val_preds = torch.cat(val_preds, dim=0)\n",
        "      val_labels = torch.cat(val_labels, dim=0)\n",
        "\n",
        "    #   print(f'{val_labels=}')\n",
        "    #   print(f'{torch.argmax(val_preds, dim=1)=}')\n",
        "\n",
        "      val_f1 = f1_score(torch.argmax(val_labels,dim=1), torch.argmax(val_preds, dim=1), average='macro')\n",
        "      if val_f1>max_val_f1:\n",
        "        max_val_f1=val_f1\n",
        "        if google_col\n",
        "        torch.save(model.state_dict(), save_directory + 'Text_model_val_f1_{:.3f}_epoch{}.ckpt'.format(val_f1,epoch))\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return(model,max_val_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8u5be1oU7yaE"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a86658183ee4407bc21619b2da98425",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8de9393dc24b189c334e03c9fcc74b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/10, Validation F1 Score: 0.0000:   0%|          | 0/874 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory /content/drive/MyDrive/Lessons/Models/multimodal_classification does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model,max_val_f1\u001b[39m=\u001b[39mtrain_model(model,train_loader,val_loader,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
            "\u001b[1;32m/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m val_f1\u001b[39m>\u001b[39mmax_val_f1:\n\u001b[1;32m     <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m       max_val_f1\u001b[39m=\u001b[39mval_f1\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m       torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Lessons/Models/multimodal_classification/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mText_model_val_f1_\u001b[39;49m\u001b[39m{:.3f}\u001b[39;49;00m\u001b[39m_epoch\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.ckpt\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(val_f1,epoch))\n\u001b[1;32m     <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m       best_model_wts \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(model\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     <a href='vscode-notebook-cell://user-gatien-374820-0.user.lab.sspcloud.fr/home/onyxia/work/multimodal_product_data_classification/notebooks/gatien_text_model_3_pytorch.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(best_model_wts)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 492\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream))\n\u001b[1;32m    462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/MyDrive/Lessons/Models/multimodal_classification does not exist."
          ]
        }
      ],
      "source": [
        "model,max_val_f1=train_model(model,train_loader,val_loader,num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNAorMrKdzAD"
      },
      "source": [
        "# Test part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coEIf5uneNo0"
      },
      "outputs": [],
      "source": [
        "X_test = pd.read_csv(filepath+'X_test.csv')\n",
        "X_test=X_test.drop(columns=\"Unnamed: 0\")\n",
        "X_test['designation'] = X_test['designation'].fillna('').apply(clean_text)\n",
        "sequences = tokenizer.texts_to_sequences(X_test['designation'])\n",
        "\n",
        "# Padding to max length of text\n",
        "data = pad_sequences(sequences, maxlen=34)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cncW6xlweu2L"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM-WMkjfgKxC"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqWFN9t5dxjm"
      },
      "outputs": [],
      "source": [
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "  for input in tqdm(data):\n",
        "      input=torch.from_numpy(input)\n",
        "      input=input.to(device)\n",
        "      input=input.unsqueeze(0)\n",
        "      val_outputs = model(input)[0]\n",
        "      val_outputs=val_outputs.detach().cpu().numpy()\n",
        "      val_preds.append(val_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFBACRoW5LMX"
      },
      "outputs": [],
      "source": [
        "val_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsEZpp8_hZmf"
      },
      "outputs": [],
      "source": [
        "val_preds=np.argmax(val_preds,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEWqgCPO6muD"
      },
      "outputs": [],
      "source": [
        "val_preds=label_encoder.inverse_transform(val_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PixKkCRskS70"
      },
      "outputs": [],
      "source": [
        "df_preds=pd.DataFrame(val_preds)\n",
        "df_preds=df_preds.set_index(df_preds.index+84916)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1dGk5Tb7Ah7"
      },
      "outputs": [],
      "source": [
        "df_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skhnz__f4xde"
      },
      "outputs": [],
      "source": [
        "df_preds.to_csv(\"eval_text_designation.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
