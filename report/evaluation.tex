\chapter{Evaluation}
\label{sec:evaluation}
\section{Comparison of different models}

\begin{equation}
	F1 = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FN} + \text{FP}}
\end{equation}

The F1 score is a statistical measure used to evaluate the precision and recall of a model's predictions, essentially capturing the balance between the model's accuracy and its comprehensiveness in identifying relevant instances. Precision represents the accuracy of positive predictions, while recall measures the model's ability to identify all actual positive instances. The F1 score is the harmonic mean of precision and recall, thus ensuring that both metrics contribute equally to the final score, with a perfect F1 score being 1 and the worst being 0 \cite{chicco-2020}.

\begin{equation}
	\text{Weighted F1} = \sum_{i=1}^{N} w_i \times F1_i
\end{equation}

The weighted F1 score extends this concept by taking into account the class imbalance within a dataset. In scenarios where some classes are more prevalent than others, the weighted F1 score calculates a separate F1 score for each class, giving each score a weight proportional to the number of true instances for each class. This approach prevents the model's performance from being overly influenced by its effectiveness on the more common classes, instead emphasizing its overall performance across all classes. The weighted F1 score is particularly valuable in datasets with significant class imbalances, as it provides a more representative assessment of the model's predictive power \cite{leung-2022}.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		Model            & weighted-F1 score \\ \hline
		CLIP             & \textbf{0.837}    \\
		Only designation & 0.779             \\
		Benchmark        & 0.814             \\
		\hline
	\end{tabular}
	\caption{Sample Table}
	\label{tab:my_label}
\end{table}

The CLIP model, which uses the designation and description field, as well as the images, got the best performance with an weighted F1 score of 0.837.



\section{Comparison of different results for the evaluation of the model}

When training the model, we took care to separate the training, valid and test sets, with a ratio of 80\% training, 10\% valid and 10\% test. The test set was not used during the training of the model and was only used for the final evaluation of the model. The valid set was used to evaluate the model during the training process. During the training of the fusion model, we got a weighted F1 score that was really high, with only one epoch, the F1 score got up to 0.950 and climbed to 0.960 at epoch 7, and this was the value on the validation set, so the result was impressive. And we thoughts we would have comparable results on the website test set.


However, when we tested the model on the website test, the weighted F1 score was only 0.837. This is a big difference between the true value and the one we got on the training and test data sets. We also note that this is not just bad luck, because we got exactly the same result when we tested a model that concatenated the results of the feature extraction instead of multiplying them.

We don't have an answer for this change in behaviour, it looks like there is some kind of data leakage during our training process, our first idea was that there were some kind of complete data leakage (train, valid and test dataset were completely mixed up), however, after careful review of the code we did not find it  but we find some source of potential answer to this phenomenon:

\subsection{Data Leakage Transfer}

When we trained the first text model, we did not paid attention that the train, test and valid dataset were the same as the one we used for the fusion model. So a potential reason for the difference in the results is that the text model contains information of the train, test and valid dataset of the fusion model, so it is easier for the fusion model to predict the result of the test dataset.

