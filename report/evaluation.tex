\chapter{Evaluation}
\label{sec:evaluation}
\section{Comparison of different models} 

\begin{equation}
	F1 = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FN} + \text{FP}}
\end{equation}

The F1 score is a statistical measure used to evaluate the precision and recall of a model's predictions, essentially capturing the balance between the model's accuracy and its comprehensiveness in identifying relevant instances. Precision represents the accuracy of positive predictions, while recall measures the model's ability to identify all actual positive instances. The F1 score is the harmonic mean of precision and recall, thus ensuring that both metrics contribute equally to the final score, with a perfect F1 score being 1 and the worst being 0 \cite{chicco-2020}.

\begin{equation}
	\text{Weighted F1} = \sum_{i=1}^{N} w_i \times F1_i
\end{equation}

The weighted F1 score extends this concept by taking into account the class imbalance within a dataset. In scenarios where some classes are more prevalent than others, the weighted F1 score calculates a separate F1 score for each class, giving each score a weight proportional to the number of true instances for each class. This approach prevents the model's performance from being overly influenced by its effectiveness on the more common classes, instead emphasizing its overall performance across all classes. The weighted F1 score is particularly valuable in datasets with significant class imbalances, as it provides a more representative assessment of the model's predictive power \cite{leung-2022}.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		Model & weighted-F1 score  \\ \hline
		CLIP & \textbf{0.837}  \\ 
		Only designation & 0.779  \\ 
		Benchmark &  0.814  \\  
		\hline
	\end{tabular}
	\caption{Sample Table}
	\label{tab:my_label}
\end{table}

The CLIP model, which uses the designation and description field, as well as the images, got the best performance with an weighted F1 score of 0.837. In contrast, the text-only model, which relies solely on product designations, achieved a weighted F1 score of 0.779. While this model benefits from the informative nature of the product titles, it lacks the additional contex that images provide. The reduction in the weighted F1 score when compared to the full CLIP model underscores the value of incorporating visual features into the classification process.

The benchmark model, established by the challenge organizers, serves as a standard for comparison, achieving a weighted F1 score of 0.814. While this model demonstrates a strong baseline performance, it falls short of the full CLIP model's capabilites.

It can be seen that integrating the images and the text data into one model gives better results than just using the text alone. It highlights the need for models that can effectively synthesize information from multiple sources, providing a more holistic view of the data at hand.

\section{Evaluation based on a confusion matrix}

A confusion matrix is a powerful tool for evaluating the performance of a classification model, providing a visual representation of the model's predictions compared against the actual labels. It not only reveals the instances of correct and incorrect predictions for each class but also uncovers the specific types of errors, such as which classes are being confused with others, enabling a nuanced analysis of the model's strengths and weaknesses \cite{susmaga-2004}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{confusion_matrix.jpg}
	\caption{Confusion matrix of CLIP model}
	\label{fig:confusionmatrix}
\end{figure}

