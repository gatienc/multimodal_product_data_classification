\chapter{Method}
\label{sec:method}

\section{Data Preparation}
Data preparation is a critical phase in the modeling process, especially for a task involving complex and diverse datasets like ours. Our data preparation process is designed to ensure that the data fed into the model is clean, structured, and suitable for effective analysis and modeling. The following sections describe the steps taken to prepare the data for the multimodal product classification task.

\subsection{Text Data Preprocessing}

To pre-process the "designation" and "description" columns, a custom word-processing function was applied. This function removed HTML tags, replaced accented characters with their corresponding unaccented counterparts, removed special characters and converted all characters to lowercase. These preprocessing steps were selected iteratively in parrallel with the evaluation of the text-feature extraction model. Some preprocessing were surprinsingly not beneficial to the model performance, such as removing stopwords.



\subsection{Categorical Encoding of Labels}

In our multimodal product classification task, the target variable 'prdtypecode' represents the category to which each product belongs. This variable is inherently categorical, meaning it represents discrete, non-numeric categories. Each unique 'prdtypecode' corresponds to a different type of product.

Categorical Encoding is used because each ‘prdtypecode’ is a distinct category with no inherent numerical relationship or order between them. Categorical encoding ensures that the model treats these categories as separate and distinct without assuming any numerical relationship or hierarchy among them \cite{potdar-2017}. If left as is, the model might misinterpret the categorical codes as ordinal or interval  data, leading to incorrect assumptions about the proximity or similarity between different categories.

For this use case One-Hot encoding was used. It converts the categorical target variable into a binary matrix. In this matrix, each category is represented by a vector where only one element is '1', and the rest are '0' \cite{cerda-2018}. This representation avoids any notion of order or magnitude among categories, which is essential for unbiased classification.
With one-hot encoding, no category is given undue preference. For instance, if numeric labels were used directly, the model might assume that higher numbers have more significance or weight, which is not the case in categorical classification.
While one-hot encoding is beneficial, it does increase the dimensionality of the dataset. Each unique category becomes a new feature in the dataset. In cases with many categories, this can lead to a large increase in the number of input features, potentially causing issues like the "curse of dimensionality"\cite{altman-2018}. However, in our context, the benefits of clear and distinct categorical representation outweigh these concerns.

\section{Visualisation}

\section{Feature Extraction}

\subsection{Image Feature Extraction}

Central to our approach is the integration of OpenAI's CLIP (Contrastive Language–Image Pretraining) model. CLIP is a foundation model based on a transformers and Vit architecture to extract text and image features in the same embedding space.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{clip_description.png}
    \caption{Clip Model presentation \\ source: pinecone.io}
    \label{fig:clip-architecture}
\end{figure}


As can be seen in the figure \ref{fig:clip-architecture}, CLIP operates on a dual-encoder structure, comprising an image encoder and a text encoder. This architecture is instrumental in processing and correlating visual and textual inputs. The model's training utilizes a contrastive learning method, where it is exposed to numerous images and their corresponding textual descriptions, drawing from a diverse and extensive internet-sourced dataset. This training enables CLIP to develop a nuanced understanding of the intricate relationships between text and images \cite{radford-2021}.
For our use case, We are going to use only the image feature extraction part of the Clip Model, hoping that the image feature extraction done by the clip encoder will provide embeddings that will be separable by the classifier.




\subsection{Textual Feature Extraction}

For the extraction of the features we used a classification model, we used CNN classifier trained both on the designation (product titles) and the non null description fields. The input size is the maximum possible designation length, 34 in this case. Shorter inputs are zero-padded. The architecture consists of an embedding layer and 6 convolutional (with ReLu), max-pooling blocks. The embeddings are trained with the entire architecture. Following is the model architecture:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{text-model-architecture.png}
    \caption{Text Model Architecture}
    \label{fig:text-model}
\end{figure}
\\

We succeeded reaching a 0.747 weighted F1-score. After the training of this model, we extracted the embeddings of the last linear layer. The embeddings are of size 768, wich match the size of the CLIP embeddings. We then used these embeddings as input for the multimodal model.

\subsection{Features Fusion}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{hateclip.png}
    \caption{Proposed architecture of Hate-CLIPper for Multimodal Hateful Meme Classification}
    \label{fig:hateclipper}
\end{figure}

\subsection{Feature Interaction Matrix}

At the core of our feature extraction process lies the Feature Interaction Matrix (FIM),wich is obtained by calculating the product between the image features anf the text features, the obtained matrices captures the intricate relationships between textual and visual features (see Figure \ref{fig:hateclipper}). This approach, as outlined in the "Hate-CLIPper" paper \cite{kumar2022hateclipper}, effectively harnesses the strengths of CLIP's robust feature extraction capabilities while expanding its ability to discern the intricate associations that simpler models might overlook. The FIM consequently enables a more holistic and contextually aware representation of the data, augmenting the model's capacity for accurate classification in complex multimodal tasks.

\section{Classification}

After flattening  the FIM, we used a simple 2 layer linear classifier to predict the prdtypecode. The classifier is a simple linear layer . The model is trained with the cross entropy loss function, Adam optimizer with a learning rate of 0.001, with a batch size of 128.