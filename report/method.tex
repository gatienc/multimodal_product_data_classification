\section{Method}
\label{sec:method}

\subsection{Data Preparation}

\subsection{Visualisation}

\subsection{Feature Extraction}

\subsection{Modelling}

\subsubsection{Integrating CLIP for Enhanced Multimodal Product Classification}
Central to our approach is the integration of OpenAI's CLIP (Contrastive Languageâ€“Image Pretraining) model. CLIP embodies a novel paradigm in artificial intelligence, harmonizing the interpretation of visual and textual data through a multimodal learning framework.

CLIP operates on a dual-encoder structure, comprising an image encoder and a text encoder. This architecture is instrumental in processing and correlating visual and textual inputs. The model's training utilizes a contrastive learning method, where it is exposed to numerous images and their corresponding textual descriptions, drawing from a diverse and extensive internet-sourced dataset. This training enables CLIP to develop a nuanced understanding of the intricate relationships between text and images.

In the realm of product classification, CLIP's integration offers a transformative potential. Our model leverages CLIP's proficiency in associating product images with their textual descriptions, such as titles and detailed narratives. This synergy allows for a more nuanced and accurate classification of products into their respective categories.


\subsection{Evaluation}