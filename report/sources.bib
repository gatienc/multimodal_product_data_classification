@misc{statista-2023,
	author = {Statista},
	month = {8},
	title = {{Rakuten Group's number of member IDs 2014-2023}},
	year = {2023},
	url = {https://www.statista.com/statistics/223349/rakuten-members/},
}

@misc{brian-2022,
	author = {Brian},
	month = {7},
	title = {{Top Largest eCommerce Companies In The World (By Revenue) - AovUp (formerly Woosuite)}},
	year = {2022},
	url = {https://aovup.com/stats/ecommerce-companies/},
}

@article{10.1093/mnrasl/slac120,
	author = {Humphrey, A and Kuberski, W and Bialek, J and Perrakis, N and Cools, W and Nuyttens, N and Elakhrass, H and Cunha, P A C},
	title = "{Machine-learning classification of astronomical sources: estimating F1-score in the absence of ground truth}",
	journal = {Monthly Notices of the Royal Astronomical Society: Letters},
	volume = {517},
	number = {1},
	pages = {L116-L120},
	year = {2022},
	month = {10},
	abstract = "{Machine-learning based classifiers have become indispensable in the field of astrophysics, allowing separation of astronomical sources into various classes, with computational efficiency suitable for application to the enormous data volumes that wide-area surveys now typically produce. In the standard supervised classification paradigm, a model is typically trained and validated using data from relatively small areas of sky, before being used to classify sources in other areas of the sky. However, population shifts between the training examples and the sources to be classified can lead to ‘silent’ degradation in model performance, which can be challenging to identify when the ground-truth is not available. In this letter, we present a novel methodology using the nannyml Confidence-Based Performance Estimation (CBPE) method to predict classifier F1-score in the presence of population shifts, but without ground-truth labels. We apply CBPE to the selection of quasars with decision-tree ensemble models, using broad-band photometry, and show that the F1-scores are predicted remarkably well (\\$\\{\\rm MAPE\\} \\sim 10\\{\\{\\ \\rm per\\ cent\\}\\}\\$; R2 = 0.74–0.92). We discuss potential use-cases in the domain of astronomy, including machine-learning model and/or hyperparameter selection, and evaluation of the suitability of training data sets for a particular classification problem.}",
	issn = {1745-3925},
	doi = {10.1093/mnrasl/slac120},
	url = {https://doi.org/10.1093/mnrasl/slac120},
	eprint = {https://academic.oup.com/mnrasl/article-pdf/517/1/L116/54610879/slac120.pdf},
}
@article{potdar-2017,
	author = {Potdar, Kedar and Pardawala, Taher S. and Pai, Chinmay D.},
	journal = {International journal of computer applications},
	month = {10},
	number = {4},
	pages = {7--9},
	title = {{A comparative study of categorical variable encoding techniques for neural network classifiers}},
	volume = {175},
	year = {2017},
	doi = {10.5120/ijca2017915495},
	url = {https://doi.org/10.5120/ijca2017915495},
}
@article{cerda-2018,
	author = {Cerda, Patricio and Varoquaux, Gaël and Kégl, Balázs},
	journal = {Machine Learning},
	month = {6},
	number = {8-10},
	pages = {1477--1494},
	title = {{Similarity encoding for learning with dirty categorical variables}},
	volume = {107},
	year = {2018},
	doi = {10.1007/s10994-018-5724-2},
	url = {https://doi.org/10.1007/s10994-018-5724-2},
}
@article{altman-2018,
	author = {Altman, Naomi and Krzywinski, Martin},
	journal = {Nature Methods},
	month = {5},
	number = {6},
	pages = {399--400},
	title = {{The curse(s) of dimensionality}},
	volume = {15},
	year = {2018},
	doi = {10.1038/s41592-018-0019-x},
	url = {https://doi.org/10.1038/s41592-018-0019-x},
}